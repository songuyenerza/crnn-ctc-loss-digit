{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main\n",
    "import random\n",
    "import argparse\n",
    "from cgi import test\n",
    "from itertools import count\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import PIL.ImageOps\n",
    "from numpy import quantile  \n",
    "\n",
    "from get_mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from cgi import test\n",
    "from itertools import count\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import PIL.ImageOps\n",
    "from numpy import quantile  \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from get_mnist import load_mnist\n",
    "from num_seq_generator import generate_numbers_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........................Gendata..........................\n",
    "for i in range(1000):\n",
    "    x= random.randint(100000,999999)\n",
    "    out_path= Path(\"D:/ANlab/sequence-generator/data\")\n",
    "    sequence= x\n",
    "    sequence = list(map(int, str(sequence)))\n",
    "    train_imgs, train_labels, _, _ = load_mnist(out_path)\n",
    "    \n",
    "    number_sequence_image = generate_numbers_sequence(\n",
    "        sequence,\n",
    "        (0,40),\n",
    "        150,\n",
    "        train_imgs,\n",
    "        train_labels,\n",
    "    )\n",
    "    im = Image.fromarray(number_sequence_image * 255)\n",
    "    im = im.convert(\"L\")\n",
    "    im = PIL.ImageOps.invert(im)\n",
    "    # print(args.sequence)\n",
    "    count= random.randint(100000000000000,999999999999999)\n",
    "    name = 'datagen_train_1000'+'/' + str(x)+\"_\" + str(count) +'.png'\n",
    "    im.save(name, quality= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout, Dense, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"D:/ANlab/sequence-generator/datagen_train_1000\"\n",
    "train_image_paths= []\n",
    "image_label_train=[]\n",
    "for path in os.listdir(data_folder):\n",
    "    train_image_paths.append(data_folder + \"/\" + path)\n",
    "    img_name = path[0:len(path)-20]\n",
    "    image_label_train.append(img_name)  \n",
    "\n",
    "len(image_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VECTOR= \"0123456789\"\n",
    "letters = [letter for letter in CHAR_VECTOR]\n",
    "\n",
    "num_classes = len(letters) + 1\n",
    "max_text_len= 6\n",
    "img_w, img_h  = 128, 64\n",
    "\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list = sorted(CHAR_VECTOR)\n",
    "def encode_to_labels(txt):\n",
    "\n",
    "    dig_lst = [] \n",
    "    for index, char in enumerate(txt):\n",
    "        try:\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except:\n",
    "            break\n",
    "            # print(char)\n",
    "    \n",
    "    return pad_sequences([dig_lst], maxlen=max_text_len, padding='post', value = 11)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_texts_train = list(map(encode_to_labels, image_label_train))\n",
    "\n",
    "print(padded_image_texts_train[100])\n",
    "image_label_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_sample(img_path, label):\n",
    "    \n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_jpeg(img, channels=1)\n",
    "\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img, [64,128])\n",
    "    img = tf.transpose(img, perm=(1,0,2))\n",
    "    label = tf.strings.as_string(label)\n",
    "    label = tf.strings.to_number(label, tf.int32)\n",
    "    return { \"Input\": img, \"Label\": label }\n",
    "    # return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainAug = Sequential([\n",
    "\t# preprocessing.Rescaling(scale=1.0 / 255),\n",
    "\n",
    "\t# preprocessing.RandomZoom(\n",
    "\t\t# height_factor=(-0.05, -0.15),\n",
    "\t\t# width_factor=(-0.05, -0.15)),\n",
    "\t# preprocessing.RandomRotation(0.01)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, padded_image_texts_train))\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        process_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    # .shuffle(buffer_size=1000)\n",
    "    # .map(lambda x, y: (trainAug(x), y),\n",
    "\t# \t num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import models\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "\n",
    "# def batch_activate(x):\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Activation('relu')(x)\n",
    "#     return x\n",
    "# def convolution_block(x,\n",
    "#                       filters,\n",
    "#                       size,\n",
    "#                       strides=(1, 1),\n",
    "#                       padding='same',\n",
    "#                       activation=True):\n",
    "#     x = layers.Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "#     if activation:\n",
    "#         x = batch_activate(x)\n",
    "#     return x\n",
    "\n",
    "# def residual_block(block_input,\n",
    "#                    num_filters=16,\n",
    "#                    use_batch_activate=False):\n",
    "#     x = batch_activate(block_input)\n",
    "#     x = convolution_block(x, num_filters, (3, 3))\n",
    "#     x = convolution_block(x, num_filters, (3, 3), activation=False)\n",
    "#     x = layers.Add()([x, block_input])\n",
    "#     if use_batch_activate:\n",
    "#         x = batch_activate(x)\n",
    "#     return x\n",
    "# def resnet_backbone(start_neurons=32,\n",
    "#                     dropout_rate=0.1):\n",
    "#     input_layer = layers.Input(\n",
    "#         name='input_image',\n",
    "#         shape=(128,32,1),\n",
    "#         dtype='float32'\n",
    "#     )\n",
    "\n",
    "#     for index, i in enumerate([1, 2, 2, 4, 8]):\n",
    "#         if index == 0:\n",
    "#             inner = input_layer\n",
    "#         inner = layers.Conv2D(start_neurons * i, (3,3),\n",
    "#                               activation=None, padding=\"same\")(inner)\n",
    "#         inner = residual_block(inner, start_neurons * i)\n",
    "#         inner = residual_block(inner, start_neurons * i, True)\n",
    "\n",
    "#         if i <=2:\n",
    "#             inner = layers.MaxPooling2D((2,2))(inner)\n",
    "\n",
    "#         if dropout_rate:\n",
    "#             inner = layers.Dropout(dropout_rate)(inner)\n",
    "    \n",
    "#     inner = Reshape(target_shape=((32,512)), name='reshape')(inner)  # (None, 32, 2048)\n",
    "#     inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)  # (None, 32, 64)\n",
    "#     inner1 = Dropout(0.1)(inner)\n",
    "\n",
    "#     x = layers.Bidirectional(\n",
    "#         layers.LSTM(units=256, return_sequences=True, dropout=0.1), name='bi_lstm1')(inner1)\n",
    "#     x = layers.Bidirectional(\n",
    "#         layers.LSTM(units=256, return_sequences=True, dropout=0.1), name='bi_lstm2')(x)\n",
    "#     x = layers.Dense(units=num_classes, name='logits')(x)\n",
    "\n",
    "    \n",
    "#     return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# model = resnet_backbone()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.layers import Reshape, Lambda, BatchNormalization\n",
    "from tensorflow.keras.layers import Add, Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# Build an endpoint layer for implementing CTC loss.\n",
    "class CTCLayer( layers.Layer ):\n",
    "    def __init__( self, name=None, **kwargs ):\n",
    "        super().__init__( name=name )\n",
    "        self.loss_fn = K.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast( tf.shape(y_true)[0], dtype='int64' )\n",
    "        input_length = tf.cast( tf.shape(y_pred)[1], dtype='int64' )\n",
    "        label_length = tf.cast( tf.shape(y_true)[1], dtype='int64' )\n",
    "        \n",
    "        input_length = input_length*tf.ones( shape=(batch_len,1), dtype='int64' )\n",
    "        label_length = label_length*tf.ones( shape=(batch_len,1), dtype='int64' )\n",
    "\n",
    "        loss = self.loss_fn( y_true, y_pred, input_length, label_length )\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "\n",
    "def get_Model(training):\n",
    "    input_shape = (128,64, 1)     # (128, 64, 1)\n",
    "    labels = Input( shape=(None,), dtype='float32', name=\"Label\" )\n",
    "    # labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "    # Make Networkw\n",
    "    inputs = Input(name='Input', shape=input_shape, dtype='float32')  # (None, 128, 64, 1)\n",
    "\n",
    "    # Convolution layer (VGG)\n",
    "    inner = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(inputs)  # (None, 128, 64, 64)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)  # (None,64, 32, 64)\n",
    "\n",
    "    inner = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)  # (None, 64, 32, 128)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)  # (None, 32, 16, 128)\n",
    "\n",
    "    inner = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)  # (None, 32, 16, 256)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(inner)  # (None, 32, 16, 256)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)  # (None, 32, 8, 256)\n",
    "\n",
    "\n",
    "    inner = Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(inner)  # (None, 32, 8, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = Conv2D(512, (3, 3), padding='same', name='conv6')(inner)  # (None, 32, 8, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(1, 2), name='max4')(inner)  # (None, 32, 4, 512)\n",
    "\n",
    "    inner = Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(inner)  # (None, 32, 4, 512)\n",
    "    inner = BatchNormalization()(inner)\n",
    "    inner = Activation('relu')(inner)\n",
    "\n",
    "\n",
    "\n",
    "    # CNN to RNN\n",
    "    inner = Reshape(target_shape=((32,2048)), name='reshape')(inner)  # (None, 32, 2048)\n",
    "    inner = Dense(128, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)  # (None, 32, 64)\n",
    "    inner1 = Dropout(0.1)(inner)\n",
    "\n",
    "\n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(units=256, return_sequences=True), name='bi_lstm1')(inner)\n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(units=256, return_sequences=True), name='bi_lstm2')(x)\n",
    "    # x = layers.Bidirectional(\n",
    "    #     layers.LSTM(units=128, return_sequences=True), name='bi_lstm3')(x)\n",
    "    x = Dense( num_classes, activation='softmax', name='Softmax' )(x) \n",
    "\n",
    "    output = CTCLayer( name='CTC_Loss' )(labels, x)\n",
    "\n",
    "    if training:\n",
    "      return Model( inputs=[inputs,labels], outputs=[output], name='CRNN_Model_with_CTC_LOSS' ) \n",
    "\n",
    "    \n",
    "model = get_Model(training=True)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile( optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CTCLoss(keras.losses.Loss):\n",
    "#     \"\"\" A class that wraps the function of tf.nn.ctc_loss. \n",
    "    \n",
    "#     Attributes:\n",
    "#         logits_time_major: If False (default) , shape is [batch, time, logits], \n",
    "#             If True, logits is shaped [time, batch, logits]. \n",
    "#         blank_index: Set the class index to use for the blank label. default is\n",
    "#             -1 (num_classes - 1). \n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, logits_time_major=False, name='ctc_loss'):\n",
    "#         super().__init__(name=name)\n",
    "#         self.logits_time_major = logits_time_major\n",
    "        \n",
    "#     def call(self, y_true, y_pred):\n",
    "#         \"\"\" \n",
    "#             Computes CTC (Connectionist Temporal Classification) loss. \n",
    "#         \"\"\"\n",
    "#         y_true = tf.cast(y_true, tf.int32)\n",
    "#         logit_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])\n",
    "#         label_length = tf.fill([tf.shape(y_true)[0]], tf.shape(y_true)[1])\n",
    "#         loss = tf.nn.ctc_loss(\n",
    "#             labels=y_true,\n",
    "#             logits=y_pred,\n",
    "#             label_length=label_length,\n",
    "#             logit_length=logit_length,\n",
    "#             logits_time_major=self.logits_time_major,\n",
    "            \n",
    "#         )\n",
    "#         return tf.math.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.get_strategy()\n",
    "# with strategy.scope():\n",
    "#     # optimizer\n",
    "#     optimizer = tf.keras.optimizers.Adam()\n",
    "#     # model=build_model()\n",
    "#     # compile\n",
    "#     model.compile(optimizer=optimizer,loss=CTCLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"D:/ANlab/sequence-generator/CP/best.hdf5\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# checkpoint_dir\n",
    "# checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, mode='min', save_best_only=True, save_weights_only=False)\n",
    "\n",
    "# callbacks_list = [checkpoint, EarlyStopping(monitor='val_loss', min_delta=0.001, patience=30, mode='min', verbose=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs = 5,\n",
    "                    # validation_data=validation_dataset,\n",
    "                    verbose = 1,\n",
    "                    # callbacks = callbacks_list,\n",
    "                    initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('D:/ANlab/sequence-generator/CP/best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('D:/ANlab/sequence-generator/CP/best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = keras.models.Model(\n",
    "    model.get_layer(name=\"Input\").input, model.get_layer(name=\"Softmax\").output\n",
    ")\n",
    "prediction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "Characters={'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=sorted(list(Characters)), num_oov_indices=0, mask_token=None )\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True )\n",
    "\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = K.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:,:6]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode('utf-8')\n",
    "        # res= res.numpy()\n",
    "        output_text.append(res)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "error= 0\n",
    "for batch in train_dataset.take(1):\n",
    "  images = batch['Input']\n",
    "  labels = batch['Label']\n",
    "  # print(images[0], labels[0])\n",
    "  preds = prediction_model.predict(images)\n",
    "  preds= decode_batch_predictions(preds)\n",
    "  orig_texts = []\n",
    "  for label in labels:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode('utf-8')\n",
    "        orig_texts.append(label)\n",
    "  # print(preds)\n",
    "  # print(orig_texts)\n",
    "  for i in range(len(orig_texts)):\n",
    "    # print(preds[i])\n",
    "    # print(orig_texts[i])\n",
    "    if preds[i] == orig_texts[i]:\n",
    "      correct+=1\n",
    "    else:\n",
    "      error += 1\n",
    "print(error)\n",
    "print(correct)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1aee8437345988c8825fe424dfed33296cb482833178e30664e8cbab21b1a22e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bops')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
